{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiple_linear_regression_new_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUFcDsdzRyw"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHReFf3_y9ms",
        "outputId": "80e68641-5a35-4ab2-ef5a-83db5523d302"
      },
      "source": [
        "!git clone https://github.com/joanby/machinelearning-az.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machinelearning-az'...\n",
            "remote: Enumerating objects: 10497, done.\u001b[K\n",
            "remote: Total 10497 (delta 0), reused 0 (delta 0), pack-reused 10497\u001b[K\n",
            "Receiving objects: 100% (10497/10497), 310.48 MiB | 12.70 MiB/s, done.\n",
            "Resolving deltas: 100% (213/213), done.\n",
            "Checking out files: 100% (10250/10250), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNKZXgtKzU2x"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gu7KWnzzUQ0",
        "outputId": "e1dfb968-e073-4239-95a9-3a493348e6b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gUxIkHWzfHV"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIQt3jBMzYRE",
        "outputId": "4d5fb46d-6b65-41c6-ba16-ff92bbee32ef"
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20200716_171004.jpg\n",
            " 20200716_171009.jpg\n",
            " 20200716_171018.jpg\n",
            " aban17.rar\n",
            " All-App-Inventor-Projects.zip\n",
            "'Análisis de Algoritmo'\n",
            " Apuntes.gdoc\n",
            " borrado_carpetas_SAPG01.txt.gdoc\n",
            "'Captura y almacenamiento'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (10).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (11).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (12).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (1).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (2).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (3).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (4).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (5).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (6).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (7).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (8).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV (9).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV - Christopher Garcia (1).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV - Christopher Garcia (2).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV - Christopher Garcia (3).pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV - Christopher Garcia.pdf'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV.docx'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV.gdoc'\n",
            "'CHRISTOPHER ALEJADRO GARCIA CISTERNA_CV.pdf'\n",
            "'Christopher Garcia (1).vcf'\n",
            "'Christopher Garcia.vcf'\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            " Commuting.gform\n",
            "'configuraciónOYM (1).txt.gdoc'\n",
            "'configuraciónOYM (2).txt.gdoc'\n",
            " configuraciónOYM.txt.gdoc\n",
            "'Contactos (1).vcf'\n",
            " Contactos.vcf\n",
            " Cristianismo.gdoc\n",
            " CV\n",
            "'CV CHRISTOPHER GARCIA (1).pdf'\n",
            "'CV CHRISTOPHER GARCIA - Christopher Garcia (1).pdf'\n",
            "'CV CHRISTOPHER GARCIA - Christopher Garcia.pdf'\n",
            "'CV Christopher Garcia.docx'\n",
            "'CV CHRISTOPHER GARCIA.pdf'\n",
            " DATA\n",
            " DEPA\n",
            "'Detalle Cuentas'\n",
            "'Documento sin título.gdoc'\n",
            " EC1.xml\n",
            "'Ecos baby'\n",
            " EC.xml\n",
            "'Ficha Postulantes New Time Group - Jefe de Datos.docx'\n",
            " FOTOS\n",
            "'Gmail - TIENES MATRICULA GRATIS DESCUENTO EN EL ARANCEL!! DIPLOMADOS FACULTAD DE INGENIERIA 2020 ONLINE LIVE.pdf'\n",
            "'HLR FNR.html'\n",
            " INACAP\n",
            " LG-RDTT_1.0.5.4.rar\n",
            "'Noviembre 2018.gdoc'\n",
            "'Permiso 2019'\n",
            "'Preparación y Evaluación de Proyecto'\n",
            "'protokoll enero y febrero.gdoc'\n",
            " Screenshot_2014-11-29-20-43-15.png\n",
            " SCRUM\n",
            " seccion01.gsheet\n",
            " seccion02.gsheet\n",
            " SPLUNK\n",
            " splunk_ivr.gsheet\n",
            " splunk_ivr.xlsx\n",
            " Temas.gdoc\n",
            "'Untitled Diagram.xml'\n",
            "'Viaje Europa 2017.xlsx'\n",
            "'VMWARE[V6]'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsK36uN0XB-"
      },
      "source": [
        "# Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTzwfUPWzrm4"
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uab9OAbV8hYN"
      },
      "source": [
        "# Instalar dependendias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qukjDgj98kE4",
        "outputId": "d9546a20-266e-483c-9bd3-68c941ab0f4b"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATUHURu12kxr"
      },
      "source": [
        "# Instalar Theano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8iWNcor2kxt",
        "outputId": "caa2eeb4-891a-4fdf-e354-72bfeea40101"
      },
      "source": [
        "!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/Theano/Theano.git\n",
            "  Cloning git://github.com/Theano/Theano.git to /tmp/pip-req-build-bh4y2vi_\n",
            "  Running command git clone -q git://github.com/Theano/Theano.git /tmp/pip-req-build-bh4y2vi_\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.5+4.gc5abd7b69-cp37-none-any.whl size=2668282 sha256=1df7e3be31e0f822cae297a8a8264a38cc33619a9e6569fcf1ed206f4a1b5727\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zqg1uj6c/wheels/ae/32/7c/62beb8371953eb20c271b3bac7d0e56e1a2020d46994346b52\n",
            "Successfully built Theano\n",
            "Installing collected packages: Theano\n",
            "  Found existing installation: Theano 1.0.5\n",
            "    Uninstalling Theano-1.0.5:\n",
            "      Successfully uninstalled Theano-1.0.5\n",
            "Successfully installed Theano-1.0.5+4.gc5abd7b69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ippBvXwr2kxt"
      },
      "source": [
        "# Instalar Tensorflow y Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-qJDnzt2kxu",
        "outputId": "9af087b3-7c66-4654-ed84-19d655f2c522"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yFpBwmNz70v"
      },
      "source": [
        "# Redes Neuronales Convolucionales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8OxSXXSz-OP"
      },
      "source": [
        "# Cómo importar las librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edZX51YLzs59"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Pcvqp72kxv"
      },
      "source": [
        "# Parte 1 - Construir el modelo de CNN# Parte 1 - Pre procesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVEdPzf4XmV"
      },
      "source": [
        "# Importar las librerías y paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9CtwK834bjy"
      },
      "source": [
        "from keras.models import Sequential\n",
        "#Sirve para crear una capa convolución 2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "#Paquete para conectar las capas de la RNN\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTSJRMK2kxw"
      },
      "source": [
        "# Inicializar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFOGy-h_2kxw"
      },
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll0VLoZQ2kxx"
      },
      "source": [
        "# Paso 1 - Convolución"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdW0G-Xg2kxx"
      },
      "source": [
        "#filters -> cantidad de mapas de caracteristicas\n",
        "#kernel_size -> detector de rasgos\n",
        "#input_shape -> tamaño de las imagenes [64, 64] y canales de color [3]\n",
        "#activation -> función para despertar la RNN\n",
        "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
        "                      input_shape = (64, 64, 3), activation = \"relu\"))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QupBCezD2kxx"
      },
      "source": [
        "# Paso 2 - Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBo-x_cU2kxy"
      },
      "source": [
        "#Recudir el tamaño del mapa de caracteristicas, en este caso de la entrada de 64 x 64... queda en 32x32. Además de estudiar rotaciones, escalado de imagenes, etc,.\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tng4DvNc2kxy"
      },
      "source": [
        "# Una segunda capa de convolución y max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REgbociH2kxy"
      },
      "source": [
        "\n",
        "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDcMPqUw2kxz"
      },
      "source": [
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhLbr2xW2kxz"
      },
      "source": [
        "# Paso 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzyQKwTa2kxz"
      },
      "source": [
        "#Aplanar el mapa de caracteristicas de Maxpooling, el cuál sirve como entrada a la RNN\n",
        "classifier.add(Flatten())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqNGyr0j2kx0"
      },
      "source": [
        "# Paso 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8CV1l02kx0"
      },
      "source": [
        "#units->valores de la capa oculta, se recomienda una elevación al cuadrado con respecto al tamaño de la imagen.\n",
        "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
        "#Como salida del algoritmo en una clasificación de perro y gatos, se debe agregar una función con salida de probabilidades\n",
        "#units = 1 -> Probabilidad si es perro o gato\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW1iBtOy2kx0"
      },
      "source": [
        "# Compilar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjLmloI2kx0"
      },
      "source": [
        "#optimizer-> optimizador de los pesos\n",
        "#loss -> función de error de coste\n",
        "#metrics -> bajo que métrica compilar\n",
        "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBuhg5WK2kx1"
      },
      "source": [
        "# Parte 2 - Ajustar la CNN a las imágenes para entrenar "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtJ4ityr2kx1"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZMvkP62kx1"
      },
      "source": [
        "#Libreria de Keras para evitar el sobreajuste de las imagenes, en donde nos permite realizar distintos tipos de cambios a las imaganes para entrenar.\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq5_z1za2kx1",
        "outputId": "29475e51-cd68-40a1-ee38-8531fc07d338"
      },
      "source": [
        "#target_size -> tamaño de las imagenes.\n",
        "#batch_size -> lote de imagenes de entrada a la RNN.\n",
        "training_dataset = train_datagen.flow_from_directory('/content/machinelearning-az/datasets/Part 8 - Deep Learning/Section 40 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
        "                                                    target_size=(64, 64),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='binary')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwUoRagZ2kx2",
        "outputId": "3c53e11c-5c16-4e60-f27c-f20933552142"
      },
      "source": [
        "testing_dataset = test_datagen.flow_from_directory('/content/machinelearning-az/datasets/Part 8 - Deep Learning/Section 40 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
        "                                                target_size=(64, 64),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ABLcnIr2kx2"
      },
      "source": [
        "# Elaborar una matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMv5t5C32kx2"
      },
      "source": [
        "#steps_per_epoch -> cantidad de imagenes en training\n",
        "#validation_steps -> cantidad de imagenes en testing\n",
        "#classifier.fit(training_dataset,\n",
        "#                        steps_per_epoch=8000,\n",
        "#                        epochs=25,\n",
        "#                        validation_data=testing_dataset,\n",
        "#                        validation_steps=2000)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx_avuha6NQA",
        "outputId": "4ffa2001-5f5e-40af-c723-76f94d969860"
      },
      "source": [
        "classifier.fit(training_dataset,\n",
        "                        steps_per_epoch=8000,\n",
        "                        epochs=25,\n",
        "                        validation_data=testing_dataset,\n",
        "                        validation_steps=2000)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            " 250/8000 [..............................] - ETA: 22:37 - loss: 0.6760 - accuracy: 0.5666WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
            "8000/8000 [==============================] - 51s 6ms/step - loss: 0.6575 - accuracy: 0.6031 - val_loss: 0.5952 - val_accuracy: 0.6715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc2ba813a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}